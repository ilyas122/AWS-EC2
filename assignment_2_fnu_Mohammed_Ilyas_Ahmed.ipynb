{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqf2fm6R++5YubxELo4Fvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilyas122/AWS-EC2/blob/main/assignment_2_fnu_Mohammed_Ilyas_Ahmed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1"
      ],
      "metadata": {
        "id": "1-XS9dE6NrIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_excel('housing.xlsx')\n",
        "X = df.iloc[:, :-1].values # features\n",
        "y = df.iloc[:, -1].values # target variable\n",
        "\n",
        "# Step 2: Normalize the features\n",
        "def feature_scaling(X):\n",
        "    return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "X = feature_scaling(X)\n",
        "\n",
        "# Step 3: Initialize the weights\n",
        "W = np.random.rand(X.shape[1])\n",
        "\n",
        "# Step 4: Define the learning rate and the number of iterations\n",
        "alpha = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Step 5: Implement the cost function and gradient descent algorithm\n",
        "def cost_function(X, y, W):\n",
        "    m = len(y)\n",
        "    J = np.sum((X.dot(W) - y) ** 2) / (2 * m)\n",
        "    return J\n",
        "\n",
        "def gradient_descent(X, y, W, alpha, epochs):\n",
        "    m = len(y)\n",
        "    J_history = np.zeros(epochs)\n",
        "    for i in range(epochs):\n",
        "        h = X.dot(W)\n",
        "        loss = h - y\n",
        "        gradient = X.T.dot(loss) / m\n",
        "        W = W - alpha * gradient\n",
        "        J_history[i] = cost_function(X, y, W)\n",
        "    return W, J_history\n",
        "\n",
        "W, J_history = gradient_descent(X, y, W, alpha, epochs)\n",
        "\n",
        "# Step 6: Plot the ‘price’ vs ‘lotsize’ using the final weight values\n",
        "plt.scatter(X[:, 0], y)\n",
        "plt.plot(X[:, 0], X.dot(W), color='red')\n",
        "plt.xlabel('lotsize')\n",
        "plt.ylabel('price')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "def rmse(X, y, W):\n",
        "    m = len(y)\n",
        "    rmse = np.sqrt(np.sum((X.dot(W) - y) ** 2) / m)\n",
        "    return rmse\n",
        "\n",
        "print('Final weight values:', W)\n",
        "print('Root Mean Squared Error:', rmse(X, y, W))\n"
      ],
      "metadata": {
        "id": "F-v2jDWLN-pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2"
      ],
      "metadata": {
        "id": "Wuiu7jmokLbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel(\"housing.xlsx\")\n",
        "\n",
        "# Select the features and target\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the features\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
        "\n",
        "# Add a column of ones to X for the intercept term\n",
        "X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n"
      ],
      "metadata": {
        "id": "UesK76eAkMYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the cost function and gradient function."
      ],
      "metadata": {
        "id": "0XRC-3x9kPTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute the cost function for linear regression.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    h = np.dot(X, theta)\n",
        "    J = (1 / (2 * m)) * np.sum((h - y) ** 2)\n",
        "    return J\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to learn theta.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    J_history = np.zeros((num_iters, 1))\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        h = np.dot(X, theta)\n",
        "        theta = theta - (alpha / m) * np.dot(X.T, (h - y))\n",
        "        J_history[i] = compute_cost(X, y, theta)\n",
        "    \n",
        "    return theta, J_history\n"
      ],
      "metadata": {
        "id": "G653PxHikRwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run the gradient descent algorithm to find the optimum weights."
      ],
      "metadata": {
        "id": "afW2NFaUkVx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the weights\n",
        "theta = np.zeros((X.shape[1], 1))\n",
        "\n",
        "# Set the learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "num_iters = 1000\n",
        "\n",
        "# Run gradient descent\n",
        "theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
        "\n",
        "# Print the final weights and RMSE\n",
        "print(\"Final weights: \\n\", theta)\n",
        "print(\"RMSE: \", np.sqrt(compute_cost(X, y, theta)))\n",
        "\n",
        "# Plot price vs lotsize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X[:, 1], y)\n",
        "plt.plot(X[:, 1], np.dot(X, theta), color=\"red\")\n",
        "plt.xlabel(\"Lotsize\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ILcVgGoNkX4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performance of the model, we can also calculate the RMSE on a test set."
      ],
      "metadata": {
        "id": "COtQLUookZ2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model on the training set\n",
        "theta = np.zeros((X.shape[1], 1))\n",
        "theta, _ = gradient_descent(X_train, y_train, theta, alpha, num_iters)\n",
        "\n",
        "# Calculate the RMSE on the test set\n",
        "rmse = np.sqrt(compute_cost(X_test, y_test, theta))\n",
        "print(\"Test RMSE: \", rmse)\n"
      ],
      "metadata": {
        "id": "4RKDg58-kdrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3"
      ],
      "metadata": {
        "id": "zTUsfpoSN_y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('housing.xlsx')\n",
        "\n",
        "# Extract the features and target variable\n",
        "X = data[['lotsize']].values\n",
        "y = data[['price']].values\n",
        "\n",
        "# Add a column of 1s to the feature array\n",
        "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "\n",
        "# Calculate the inverse of the product of the feature matrix and its transpose\n",
        "XTX_inv = np.linalg.inv(np.dot(X.T, X))\n",
        "\n",
        "# Calculate the product of the inverse and the target variable array to obtain the weights\n",
        "w = np.dot(np.dot(XTX_inv, X.T), y)\n",
        "\n",
        "# Calculate the predicted target variable values\n",
        "y_pred = np.dot(X, w)\n",
        "\n",
        "# Calculate the root mean squared error (RMSE)\n",
        "rmse = np.sqrt(np.mean((y_pred - y) ** 2))\n",
        "\n",
        "# Print the weight values and RMSE\n",
        "print('Weight values:', w)\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "# Plot the data and the linear regression line\n",
        "plt.scatter(X[:,1], y)\n",
        "plt.plot(X[:,1], y_pred, color='red')\n",
        "plt.xlabel('Lotsize')\n",
        "plt.ylabel('Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0uwUqpaIgim8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4"
      ],
      "metadata": {
        "id": "deGZwOLlhrmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Load the data from the Excel file\n",
        "df = pd.read_excel('housing.xlsx')\n",
        "\n",
        "# Extract input and output features\n",
        "X = df[['lotsize']]\n",
        "y = df['price']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Create a LinearRegression model object\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the output for the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the RMSE for the predictions\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Print the weight values\n",
        "print(\"Weights:\", model.coef_)\n",
        "\n",
        "# Plot the linear regression line and the data points\n",
        "plt.scatter(X_test, y_test, color='black')\n",
        "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
        "plt.xlabel('lotsize')\n",
        "plt.ylabel('price')\n",
        "plt.title('Linear Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3sCQSms7hnIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1"
      ],
      "metadata": {
        "id": "9SV1pUniiP8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the data from the Excel file\n",
        "df = pd.read_excel('housing.xlsx')\n",
        "\n",
        "# Extract input and output features\n",
        "X = df[['lotsize']].values\n",
        "y = df['price'].values\n",
        "\n",
        "# Split the dataset into training, validation, and testing sets\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=0)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define a function to compute the polynomial features up to 16th order\n",
        "def polynomial_features(X, order):\n",
        "    X_poly = X.copy()\n",
        "    for i in range(2, order+1):\n",
        "        X_poly = np.concatenate((X_poly, np.power(X, i)), axis=1)\n",
        "    return X_poly\n",
        "\n",
        "# Define a function to compute the mean squared error (MSE) with Ridge regularization\n",
        "def mse_ridge(X, y, w, l2_penalty):\n",
        "    y_pred = np.dot(X, w)\n",
        "    mse = np.mean((y - y_pred) ** 2) + l2_penalty * np.dot(w, w)\n",
        "    return mse\n",
        "\n",
        "# Define a function to compute the gradient of the MSE with Ridge regularization\n",
        "def gradient_mse_ridge(X, y, w, l2_penalty):\n",
        "    y_pred = np.dot(X, w)\n",
        "    grad = -2 * np.dot(X.T, (y - y_pred)) + 2 * l2_penalty * w\n",
        "    return grad\n",
        "\n",
        "# Define a function to perform batch gradient descent\n",
        "def batch_gradient_descent(X, y, initial_w, learning_rate, l2_penalty, num_iterations):\n",
        "    w = initial_w.copy()\n",
        "    for i in range(num_iterations):\n",
        "        grad = gradient_m\n"
      ],
      "metadata": {
        "id": "5oXFmjKBiStl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2"
      ],
      "metadata": {
        "id": "DVU1Oi6ViTjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_excel('housing.xlsx')\n",
        "df = df.dropna()\n",
        "\n",
        "X = df[['lotsize']].values\n",
        "y = df[['price']].values\n",
        "\n",
        "# Data normalization\n",
        "X_mean = np.mean(X)\n",
        "X_std = np.std(X)\n",
        "y_mean = np.mean(y)\n",
        "y_std = np.std(y)\n",
        "\n",
        "X = (X - X_mean) / X_std\n",
        "y = (y - y_mean) / y_std\n",
        "\n",
        "# Train-validation-test split\n",
        "n = X.shape[0]\n",
        "idx = np.arange(n)\n",
        "np.random.seed(0)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "train_idx = idx[:int(0.6*n)]\n",
        "valid_idx = idx[int(0.6*n):int(0.8*n)]\n",
        "test_idx = idx[int(0.8*n):]\n",
        "\n",
        "X_train = X[train_idx]\n",
        "y_train = y[train_idx]\n",
        "\n",
        "X_valid = X[valid_idx]\n",
        "y_valid = y[valid_idx]\n",
        "\n",
        "X_test = X[test_idx]\n",
        "y_test = y[test_idx]\n",
        "\n",
        "def polynomial_features(X, degree):\n",
        "    n_samples, n_features = np.shape(X)\n",
        "\n",
        "    def index_combinations():\n",
        "        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
        "        flat_combs = [item for sublist in combs for item in sublist]\n",
        "        return flat_combs\n",
        "\n",
        "    combinations = index_combinations()\n",
        "    n_output_features = len(combinations)\n",
        "    X_new = np.empty((n_samples, n_output_features))\n",
        "\n",
        "    for i, index_combs in enumerate(combinations):\n",
        "        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n",
        "\n",
        "    return X_new\n",
        "\n",
        "def compute_cost(X, y, W, lam):\n",
        "    m = len(y)\n",
        "    h = np.dot(X, W)\n",
        "    mse = np.sum((h-y)**2)/(2*m)\n",
        "    l1_reg = lam * np.sum(np.abs(W))\n",
        "    cost = mse + l1_reg\n",
        "    return cost\n",
        "\n",
        "def step_gradient(X, y, W, alpha, lam):\n",
        "    m = len(y)\n",
        "    h = np.dot(X, W)\n",
        "    loss = h-y\n",
        "\n",
        "    grad = np.dot(X.T, loss)/m\n",
        "    grad = grad + (lam * np.sign(W))/m\n",
        "\n",
        "    W = W - alpha*grad\n",
        "\n",
        "    return W\n",
        "\n",
        "def lasso_gradient_descent(X, y, alpha=0.01, lam=0.01, iterations=1000):\n",
        "    W = np.zeros((X.shape[1], 1))\n",
        "    m = len(y)\n",
        "\n",
        "    cost_history = []\n",
        "    W_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        W = step_gradient(X, y, W, alpha, lam)\n",
        "\n",
        "        cost = compute_cost(X, y, W, lam)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        W_history.append(W.flatten())\n",
        "\n",
        "    return W, cost_history, W_history\n",
        "\n"
      ],
      "metadata": {
        "id": "3cHdXqhoixi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define a function to search for the optimum L1 penalty based on the RMSE of the validation data."
      ],
      "metadata": {
        "id": "3ONZ5teii-Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_optimum_l1_penalty(X_train, y_train, X_valid, y_valid):\n",
        "    alpha = 0.01\n",
        "    iterations = 1000\n",
        "    l1_penalties = [0\n"
      ],
      "metadata": {
        "id": "mBWes7triyio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}